{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper-tf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-tf.py\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "path = 'stop_words_en.txt'\n",
    "tagWord = 'wordN'\n",
    "tagWords = 'totalWordsNt'\n",
    "\n",
    "stopWords = set()\n",
    "\n",
    "with open(path) as stopWordsFile:\n",
    "    for line in stopWordsFile:\n",
    "        try:         \n",
    "            stopWords.add(unicode(line.strip().lower()))\n",
    "        except ValueError as e:\n",
    "            continue\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = unicode(line.strip()).split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        print(\"reporter:counter:Wiki stats,Wrong articles found,%d\" % 1, file=sys.stderr)\n",
    "        continue\n",
    "\n",
    "    text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "    words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "    \n",
    "    wordsInArticle = 0\n",
    "    \n",
    "    for word in words:\n",
    "        word = word.strip(string.punctuation).lower()\n",
    "        if word == \"\":\n",
    "            print(\"reporter:counter:Wiki stats,Null words found,%d\" % 1, file=sys.stderr)\n",
    "            continue\n",
    "        if word in stopWords:\n",
    "            print(\"reporter:counter:Wiki stats,Stop words found,%d\" % 1, file=sys.stderr)\n",
    "            continue\n",
    "        wordsInArticle += 1\n",
    "        print(\"reporter:counter:Wiki stats,Total words found,%d\" % 1, file=sys.stderr)\n",
    "        print(article_id, tagWord, word, 1, sep='\\t')\n",
    "\n",
    "    print(article_id, tagWords, \"-\", wordsInArticle, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer-tf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-tf.py\n",
    "import sys\n",
    "\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "current_article_id = None\n",
    "current_word = None\n",
    "word_sum, wordsInArticle = 0, 0\n",
    "\n",
    "tagWord = 'wordN'\n",
    "tagWords = 'totalWordsNt'\n",
    "tagTf = 'tf'\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, tag, word, count = unicode(line.strip()).split('\\t')\n",
    "        #print \"after parse:\", article_id, word, count\n",
    "        if article_id == \"\" or tag == \"\" or word == \"\" or count == \"\":   # sanity check\n",
    "            continue\n",
    "        count = int(count)\n",
    "    except ValueError as e:\n",
    "        #print \"Parse error:\", e\n",
    "        continue\n",
    "\n",
    "    if current_article_id != article_id:\n",
    "        if tag == tagWords: # get total words in article\n",
    "            wordsInArticle = count\n",
    "            continue\n",
    "        if current_article_id != None:\n",
    "            print \"%s\\t%s\\t%s\\t%f\" % (current_word, tagTf, current_article_id, 1.0*word_sum/wordsInArticle)\n",
    "            word_sum = 0\n",
    "            current_word = word\n",
    "        current_article_id = article_id\n",
    "        #print \"wordsInArticle:\", word, wordsInArticle\n",
    "        \n",
    "    if current_word != word:\n",
    "        if current_word != None:\n",
    "            print \"%s\\t%s\\t%s\\t%f\" % (current_word, tagTf, current_article_id, 1.0*word_sum/wordsInArticle)\n",
    "            #print \"%s\\t%s\\t%d\\t%d\" % (current_article_id, current_word, word_sum, wordsInArticle)\n",
    "        word_sum = 0\n",
    "        current_word = word\n",
    "    word_sum += count\n",
    "\n",
    "if current_article_id:\n",
    "    print \"%s\\t%s\\t%s\\t%f\" % (current_word, tagTf, current_article_id, 1.0*word_sum/wordsInArticle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper-idf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-idf.py\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "for line in sys.stdin:\n",
    "    word, _, _, _ = unicode(line.strip()).split('\\t')\n",
    "    print(\"reporter:counter:Idf stats,Total words found,%d\" % 1, file=sys.stderr)\n",
    "    print(word, 1, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer-idf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-idf.py\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "current_key = None\n",
    "word_sum = 0\n",
    "tag = \"idf\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        key, count = unicode(line.strip()).split('\\t', 1)\n",
    "        if key == \"\" or count == \"\":    # sanity check\n",
    "            continue\n",
    "        count = int(count)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    if current_key != key:\n",
    "        if current_key:\n",
    "            print \"%s\\t%s\\t%f\" % (current_key, tag, (1.0/log(1.0 + word_sum)))\n",
    "        word_sum = 0\n",
    "        current_key = key\n",
    "    word_sum += count\n",
    "\n",
    "if current_key:\n",
    "    print \"%s\\t%s\\t%f\" % (current_key, tag, (1.0/log(1.0 + word_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper-tf-idf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-tf-idf.py\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "tf = \"tf\"\n",
    "idf = \"idf\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    inVal = unicode(line.strip()).split('\\t')\n",
    "    if len(inVal) > 4:\n",
    "        print(\"reporter:counter:Wiki stats,tf idf > 4 parsing errors,%d\" % 1, file=sys.stderr)\n",
    "        continue\n",
    "    try:\n",
    "        if inVal[1] == idf: # idf data\n",
    "            print(\"reporter:counter:Wiki stats,idf words,%d\" % 1, file=sys.stderr)\n",
    "            print(inVal[0], idf, float(inVal[2]), sep='\\t')\n",
    "            \n",
    "        elif inVal[1] == tf: # tf data\n",
    "            print(\"reporter:counter:Wiki stats,tf words,%d\" % 1, file=sys.stderr)\n",
    "            print(inVal[0], tf, inVal[2], float(inVal[3]), sep='\\t')\n",
    "            \n",
    "        else:\n",
    "            print(\"reporter:counter:Wiki stats,tf idf last else parsing errors,%d\" % 1, file=sys.stderr)\n",
    "    except ValueError as e:\n",
    "        print(\"reporter:counter:Wiki stats,tf idf to float errors,%d\" % 1, file=sys.stderr)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer-tf-idf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-tf-idf.py\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "tf = \"tf\"\n",
    "idf = \"idf\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    inVal = unicode(line.strip()).split('\\t')\n",
    "    if (len(inVal) < 2 and len(inVal) > 4):      # sanity check\n",
    "        continue\n",
    "    try:\n",
    "        if inVal[1] == idf:\n",
    "            idfWord, _, idfDt = inVal\n",
    "            idfDt = float(idfDt)\n",
    "\n",
    "        if inVal[1] == tf:\n",
    "            tfWord, _, article_id, tfNt = inVal\n",
    "            tfNt = float(tfNt)\n",
    "            if idfWord == tfWord:\n",
    "                print \"%s\\t%s\\t%f\" % (idfWord, article_id, tfNt*idfDt)\n",
    "\n",
    "    except ValueError as e:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put wiki-test.txt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrwxrwxrwx 1 root root 61 Oct 17  2017 /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar -> /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.8.1.jar\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar981246973952476236/] [] /tmp/streamjob1345325328332772262.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/02/08 07:13:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/02/08 07:13:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/02/08 07:13:50 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "19/02/08 07:13:50 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/02/08 07:13:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1549545179555_0023\n",
      "19/02/08 07:13:50 INFO impl.YarnClientImpl: Submitted application application_1549545179555_0023\n",
      "19/02/08 07:13:50 INFO mapreduce.Job: The url to track the job: http://e5801941607d:8088/proxy/application_1549545179555_0023/\n",
      "19/02/08 07:13:50 INFO mapreduce.Job: Running job: job_1549545179555_0023\n",
      "19/02/08 07:13:56 INFO mapreduce.Job: Job job_1549545179555_0023 running in uber mode : false\n",
      "19/02/08 07:13:56 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/02/08 07:14:13 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "19/02/08 07:14:19 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "19/02/08 07:14:25 INFO mapreduce.Job:  map 34% reduce 0%\n",
      "19/02/08 07:14:31 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "19/02/08 07:14:38 INFO mapreduce.Job:  map 49% reduce 0%\n",
      "19/02/08 07:14:44 INFO mapreduce.Job:  map 55% reduce 0%\n",
      "19/02/08 07:14:50 INFO mapreduce.Job:  map 61% reduce 0%\n",
      "19/02/08 07:14:55 INFO mapreduce.Job:  map 64% reduce 0%\n",
      "19/02/08 07:14:56 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "19/02/08 07:15:01 INFO mapreduce.Job:  map 98% reduce 0%\n",
      "19/02/08 07:15:02 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/02/08 07:15:26 INFO mapreduce.Job:  map 100% reduce 85%\n",
      "19/02/08 07:15:31 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "19/02/08 07:15:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/02/08 07:15:34 INFO mapreduce.Job: Job job_1549545179555_0023 completed successfully\n",
      "19/02/08 07:15:35 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=316786759\n",
      "\t\tFILE: Number of bytes written=475859882\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=76874501\n",
      "\t\tHDFS: Number of bytes written=87166718\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=127164\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=78407\n",
      "\t\tTotal time spent by all map tasks (ms)=127164\n",
      "\t\tTotal time spent by all reduce tasks (ms)=78407\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=127164\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=78407\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=130215936\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=80288768\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4100\n",
      "\t\tMap output records=6975047\n",
      "\t\tMap output bytes=144435146\n",
      "\t\tMap output materialized bytes=158385291\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3476303\n",
      "\t\tReduce shuffle bytes=158385291\n",
      "\t\tReduce input records=6975047\n",
      "\t\tReduce output records=3472203\n",
      "\t\tSpilled Records=20925141\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=7745\n",
      "\t\tCPU time spent (ms)=217210\n",
      "\t\tPhysical memory (bytes) snapshot=1247432704\n",
      "\t\tVirtual memory (bytes) snapshot=9830121472\n",
      "\t\tTotal committed heap usage (bytes)=903872512\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tWiki stats\n",
      "\t\tNull words found=72\n",
      "\t\tStop words found=4966298\n",
      "\t\tTotal words found=6970947\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=76874273\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=87166718\n",
      "19/02/08 07:15:35 INFO streaming.StreamJob: Output directory: tf_result\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUT_DIR_TF=\"tf_result\"\n",
    "#OUT_DIR=\"/data/wiki/test/tf-out\"\n",
    "NUM_REDUCERS=3\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR_TF} > /dev/null\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D stream.num.reduce.output.fields=3 \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-D mapreduce.partition.keycomparator.options=-k1,3 \\\n",
    "-D mapreduce.job.name=\"Tf-Idf tf task\" \\\n",
    "-D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "-files mapper-tf.py,reducer-tf.py,/datasets/stop_words_en.txt  \\\n",
    "-mapper \"python mapper-tf.py\" \\\n",
    "-reducer \"python reducer-tf.py\" \\\n",
    "-input /data/wiki/en_articles_part \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-output ${OUT_DIR_TF} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-396-08747-7\ttf\t1002\t0.000578\r\n",
      "1\ttf\t1002\t0.003470\r\n",
      "11\ttf\t1002\t0.000578\r\n",
      "12\ttf\t1002\t0.000578\r\n",
      "13\ttf\t1002\t0.000578\r\n",
      "14\ttf\t1002\t0.000578\r\n",
      "15\ttf\t1002\t0.000578\r\n",
      "16\ttf\t1002\t0.000578\r\n",
      "1863\ttf\t1002\t0.000578\r\n",
      "1869\ttf\t1002\t0.000578\r\n",
      "1926\ttf\t1002\t0.001157\r\n",
      "1927\ttf\t1002\t0.000578\r\n",
      "1930\ttf\t1002\t0.001735\r\n",
      "1932\ttf\t1002\t0.001157\r\n",
      "1939\ttf\t1002\t0.000578\r\n",
      "1940\ttf\t1002\t0.001157\r\n",
      "1940s\ttf\t1002\t0.000578\r\n",
      "1942\ttf\t1002\t0.000578\r\n",
      "1943\ttf\t1002\t0.000578\r\n",
      "1950\ttf\t1002\t0.001735\r\n",
      "1952\ttf\t1002\t0.001735\r\n",
      "1953\ttf\t1002\t0.001157\r\n",
      "1954\ttf\t1002\t0.000578\r\n",
      "1956\ttf\t1002\t0.000578\r\n",
      "1957\ttf\t1002\t0.001735\r\n",
      "1960\ttf\t1002\t0.000578\r\n",
      "1960s\ttf\t1002\t0.000578\r\n",
      "1961\ttf\t1002\t0.000578\r\n",
      "1962\ttf\t1002\t0.001157\r\n",
      "1963\ttf\t1002\t0.000578\r\n",
      "1964\ttf\t1002\t0.002313\r\n",
      "1965\ttf\t1002\t0.000578\r\n",
      "1965).the\ttf\t1002\t0.000578\r\n",
      "1970\ttf\t1002\t0.000578\r\n",
      "1971\ttf\t1002\t0.000578\r\n",
      "1974\ttf\t1002\t0.000578\r\n",
      "1976\ttf\t1002\t0.001157\r\n",
      "1977\ttf\t1002\t0.000578\r\n",
      "1979\ttf\t1002\t0.000578\r\n",
      "1980\ttf\t1002\t0.000578\r\n",
      "1983\ttf\t1002\t0.001157\r\n",
      "1984\ttf\t1002\t0.001735\r\n",
      "1985\ttf\t1002\t0.002313\r\n",
      "1986\ttf\t1002\t0.000578\r\n",
      "1987\ttf\t1002\t0.002313\r\n",
      "1989\ttf\t1002\t0.000578\r\n",
      "1991\ttf\t1002\t0.000578\r\n",
      "1992\ttf\t1002\t0.001157\r\n",
      "1993\ttf\t1002\t0.000578\r\n",
      "1993-2001\ttf\t1002\t0.000578\r\n",
      "1995\ttf\t1002\t0.001157\r\n",
      "1997\ttf\t1002\t0.001157\r\n",
      "1998\ttf\t1002\t0.001157\r\n",
      "1999\ttf\t1002\t0.001157\r\n",
      "20\ttf\t1002\t0.002313\r\n",
      "2001\ttf\t1002\t0.002313\r\n",
      "2004\ttf\t1002\t0.002892\r\n",
      "2005\ttf\t1002\t0.001735\r\n",
      "2006\ttf\t1002\t0.001735\r\n",
      "2007\ttf\t1002\t0.001157\r\n",
      "2008\ttf\t1002\t0.001735\r\n",
      "2009\ttf\t1002\t0.002892\r\n",
      "2010\ttf\t1002\t0.002313\r\n",
      "21-24\ttf\t1002\t0.000578\r\n",
      "22\ttf\t1002\t0.000578\r\n",
      "23\ttf\t1002\t0.000578\r\n",
      "243\ttf\t1002\t0.000578\r\n",
      "25–29\ttf\t1002\t0.000578\r\n",
      "26–30\ttf\t1002\t0.000578\r\n",
      "27\ttf\t1002\t0.001157\r\n",
      "29\ttf\t1002\t0.001157\r\n",
      "3\ttf\t1002\t0.000578\r\n",
      "30\ttf\t1002\t0.001157\r\n",
      "30-33\ttf\t1002\t0.000578\r\n",
      "36\ttf\t1002\t0.000578\r\n",
      "39\ttf\t1002\t0.000578\r\n",
      "4\ttf\t1002\t0.002313\r\n",
      "4.50\ttf\t1002\t0.002313\r\n",
      "41\ttf\t1002\t0.000578\r\n",
      "4:50\ttf\t1002\t0.001157\r\n",
      "5\ttf\t1002\t0.004049\r\n",
      "6\ttf\t1002\t0.000578\r\n",
      "7\ttf\t1002\t0.000578\r\n",
      "70\ttf\t1002\t0.000578\r\n",
      "8\ttf\t1002\t0.000578\r\n",
      "9\ttf\t1002\t0.001157\r\n",
      "ability\ttf\t1002\t0.000578\r\n",
      "abney\ttf\t1002\t0.000578\r\n",
      "accessed\ttf\t1002\t0.000578\r\n",
      "account\ttf\t1002\t0.000578\r\n",
      "ackroyd\ttf\t1002\t0.001157\r\n",
      "acquaintances\ttf\t1002\t0.000578\r\n",
      "actress\ttf\t1002\t0.002313\r\n",
      "acts\ttf\t1002\t0.000578\r\n",
      "actually\ttf\t1002\t0.000578\r\n",
      "acuity\ttf\t1002\t0.000578\r\n",
      "adams\ttf\t1002\t0.000578\r\n",
      "adaptation\ttf\t1002\t0.001735\r\n",
      "adaptations\ttf\t1002\t0.001735\r\n",
      "adapted\ttf\t1002\t0.001735\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat tf_result/* | head -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -getmerge tf_result/* tf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worsen\ttf\t986\t0.000361\r\n",
      "worsening\ttf\t986\t0.000361\r\n",
      "write\ttf\t986\t0.000361\r\n",
      "writing\ttf\t986\t0.000722\r\n",
      "written\ttf\t986\t0.000361\r\n",
      "year\ttf\t986\t0.000361\r\n",
      "years\ttf\t986\t0.000722\r\n",
      "yield\ttf\t986\t0.000361\r\n",
      "young\ttf\t986\t0.001445\r\n",
      "zest\ttf\t986\t0.000361\r\n"
     ]
    }
   ],
   "source": [
    "!tail tf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `idf_result': No such file or directory\n",
      "19/02/08 07:20:22 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/02/08 07:20:22 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/02/08 07:20:23 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "19/02/08 07:20:24 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "19/02/08 07:20:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1549545179555_0024\n",
      "19/02/08 07:20:25 INFO impl.YarnClientImpl: Submitted application application_1549545179555_0024\n",
      "19/02/08 07:20:25 INFO mapreduce.Job: The url to track the job: http://e5801941607d:8088/proxy/application_1549545179555_0024/\n",
      "19/02/08 07:20:25 INFO mapreduce.Job: Running job: job_1549545179555_0024\n",
      "19/02/08 07:20:32 INFO mapreduce.Job: Job job_1549545179555_0024 running in uber mode : false\n",
      "19/02/08 07:20:32 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/02/08 07:20:50 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "19/02/08 07:20:51 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/02/08 07:21:01 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "19/02/08 07:21:03 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/02/08 07:21:03 INFO mapreduce.Job: Job job_1549545179555_0024 completed successfully\n",
      "19/02/08 07:21:03 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=42248818\n",
      "\t\tFILE: Number of bytes written=85334735\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=87167036\n",
      "\t\tHDFS: Number of bytes written=9990463\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=47929\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=25651\n",
      "\t\tTotal time spent by all map tasks (ms)=47929\n",
      "\t\tTotal time spent by all reduce tasks (ms)=25651\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=47929\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=25651\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=49079296\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=26266624\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3472203\n",
      "\t\tMap output records=3472203\n",
      "\t\tMap output bytes=35304384\n",
      "\t\tMap output materialized bytes=42248854\n",
      "\t\tInput split bytes=318\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=426169\n",
      "\t\tReduce shuffle bytes=42248854\n",
      "\t\tReduce input records=3472203\n",
      "\t\tReduce output records=426169\n",
      "\t\tSpilled Records=6944406\n",
      "\t\tShuffled Maps =9\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=9\n",
      "\t\tGC time elapsed (ms)=962\n",
      "\t\tCPU time spent (ms)=30520\n",
      "\t\tPhysical memory (bytes) snapshot=1366970368\n",
      "\t\tVirtual memory (bytes) snapshot=11782123520\n",
      "\t\tTotal committed heap usage (bytes)=932708352\n",
      "\tIdf stats\n",
      "\t\tTotal words found=3472203\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=87166718\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9990463\n",
      "19/02/08 07:21:03 INFO streaming.StreamJob: Output directory: idf_result\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#OUT_DIR_IDF=\"idf_result_\"$(date +\"%s%6N\")\n",
    "OUT_DIR_TF=\"tf_result\"\n",
    "OUT_DIR_IDF=\"idf_result\"\n",
    "NUM_REDUCERS=3\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR_IDF} > /dev/null\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"Tf-Idf idf task\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -files mapper-idf.py,reducer-idf.py \\\n",
    "    -mapper \"python mapper-idf.py\" \\\n",
    "    -reducer \"python reducer-idf.py\" \\\n",
    "    -input ${OUT_DIR_TF} \\\n",
    "    -output ${OUT_DIR_IDF} > /dev/null\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   1 jovyan supergroup          0 2019-02-08 07:21 idf_result/_SUCCESS\r\n",
      "-rw-r--r--   1 jovyan supergroup    3326152 2019-02-08 07:20 idf_result/part-00000\r\n",
      "-rw-r--r--   1 jovyan supergroup    3338790 2019-02-08 07:21 idf_result/part-00001\r\n",
      "-rw-r--r--   1 jovyan supergroup    3325521 2019-02-08 07:21 idf_result/part-00002\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls idf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -getmerge idf_result/* idf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%however\tidf\t1.442695\r\n",
      "0&\\mathrm{if\tidf\t1.442695\r\n",
      "0&\\mbox{if\tidf\t1.442695\r\n",
      "0)).(1\tidf\t1.442695\r\n",
      "0).in\tidf\t1.442695\r\n",
      "0).one\tidf\t1.442695\r\n",
      "0+1\tidf\t1.442695\r\n",
      "0,0\tidf\t0.455120\r\n",
      "0,00\tidf\t1.442695\r\n",
      "0,03\tidf\t1.442695\r\n"
     ]
    }
   ],
   "source": [
    "! head idf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "𐑈\tidf\t1.442695\r\n",
      "𐑋\tidf\t1.442695\r\n",
      "𐑎\tidf\t1.442695\r\n",
      "𐡁\tidf\t1.442695\r\n",
      "𐡄\tidf\t1.442695\r\n",
      "𐡇\tidf\t1.442695\r\n",
      "𐡊\tidf\t1.442695\r\n",
      "𐡍\tidf\t1.442695\r\n",
      "𐡐\tidf\t1.442695\r\n",
      "𐡓\tidf\t1.442695\r\n"
     ]
    }
   ],
   "source": [
    "! tail idf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `tf-idf-out': No such file or directory\n",
      "19/02/08 07:35:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/02/08 07:35:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/02/08 07:35:52 INFO mapred.FileInputFormat: Total input files to process : 6\n",
      "19/02/08 07:35:52 INFO mapreduce.JobSubmitter: number of splits:6\n",
      "19/02/08 07:35:53 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1549545179555_0025\n",
      "19/02/08 07:35:53 INFO impl.YarnClientImpl: Submitted application application_1549545179555_0025\n",
      "19/02/08 07:35:53 INFO mapreduce.Job: The url to track the job: http://e5801941607d:8088/proxy/application_1549545179555_0025/\n",
      "19/02/08 07:35:53 INFO mapreduce.Job: Running job: job_1549545179555_0025\n",
      "19/02/08 07:35:59 INFO mapreduce.Job: Job job_1549545179555_0025 running in uber mode : false\n",
      "19/02/08 07:35:59 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/02/08 07:36:28 INFO mapreduce.Job: Task Id : attempt_1549545179555_0025_m_000002_0, Status : FAILED\n",
      "Container [pid=17208,containerID=container_1549545179555_0025_01_000004] is running beyond virtual memory limits. Current usage: 495.7 MB of 1 GB physical memory used; 3.6 GB of 2.1 GB virtual memory used. Killing container.\n",
      "Dump of the process-tree for container_1549545179555_0025_01_000004 :\n",
      "\t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n",
      "\t|- 17456 17221 17208 17208 (java) 0 0 1947484160 63148 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000004/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000004 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000002_0 4 \n",
      "\t|- 17221 17208 17208 17208 (java) 440 28 1947484160 63148 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000004/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000004 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000002_0 4 \n",
      "\t|- 17208 17206 17208 17208 (bash) 0 0 11550720 592 /bin/bash -c /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000004/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000004 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000002_0 4 1>/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000004/stdout 2>/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000004/stderr  \n",
      "\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "19/02/08 07:36:29 INFO mapreduce.Job: Task Id : attempt_1549545179555_0025_m_000003_0, Status : FAILED\n",
      "Container [pid=17209,containerID=container_1549545179555_0025_01_000005] is running beyond virtual memory limits. Current usage: 494.9 MB of 1 GB physical memory used; 3.6 GB of 2.1 GB virtual memory used. Killing container.\n",
      "Dump of the process-tree for container_1549545179555_0025_01_000005 :\n",
      "\t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n",
      "\t|- 17233 17209 17209 17209 (java) 438 25 1949130752 63056 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000005/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000005 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000003_0 5 \n",
      "\t|- 17465 17233 17209 17209 (java) 0 0 1949130752 63056 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000005/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000005 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000003_0 5 \n",
      "\t|- 17209 17205 17209 17209 (bash) 0 0 11550720 582 /bin/bash -c /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000005/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000005 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000003_0 5 1>/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000005/stdout 2>/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000005/stderr  \n",
      "\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "19/02/08 07:36:29 INFO mapreduce.Job: Task Id : attempt_1549545179555_0025_m_000000_0, Status : FAILED\n",
      "Container [pid=17223,containerID=container_1549545179555_0025_01_000002] is running beyond virtual memory limits. Current usage: 451.6 MB of 1 GB physical memory used; 3.6 GB of 2.1 GB virtual memory used. Killing container.\n",
      "Dump of the process-tree for container_1549545179555_0025_01_000002 :\n",
      "\t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n",
      "\t|- 17223 17211 17223 17223 (bash) 0 0 11550720 584 /bin/bash -c /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000000_0 2 1>/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000002/stdout 2>/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000002/stderr  \n",
      "\t|- 17235 17223 17223 17223 (java) 445 34 1949360128 57512 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000000_0 2 \n",
      "\t|- 17462 17235 17223 17223 (java) 0 0 1949360128 57512 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000000_0 2 \n",
      "\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "19/02/08 07:36:29 INFO mapreduce.Job: Task Id : attempt_1549545179555_0025_m_000001_0, Status : FAILED\n",
      "Container [pid=17248,containerID=container_1549545179555_0025_01_000003] is running beyond virtual memory limits. Current usage: 497.3 MB of 1 GB physical memory used; 3.6 GB of 2.1 GB virtual memory used. Killing container.\n",
      "Dump of the process-tree for container_1549545179555_0025_01_000003 :\n",
      "\t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n",
      "\t|- 17277 17248 17248 17248 (java) 456 32 1946750976 63352 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000003/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000003 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000001_0 3 \n",
      "\t|- 17463 17277 17248 17248 (java) 0 0 1946750976 63352 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000003/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000003 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000001_0 3 \n",
      "\t|- 17248 17243 17248 17248 (bash) 0 0 11550720 592 /bin/bash -c /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000003/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000003 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000001_0 3 1>/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000003/stdout 2>/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000003/stderr  \n",
      "\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "19/02/08 07:36:29 INFO mapreduce.Job: Task Id : attempt_1549545179555_0025_m_000004_0, Status : FAILED\n",
      "Container [pid=17262,containerID=container_1549545179555_0025_01_000006] is running beyond virtual memory limits. Current usage: 452.9 MB of 1 GB physical memory used; 3.6 GB of 2.1 GB virtual memory used. Killing container.\n",
      "Dump of the process-tree for container_1549545179555_0025_01_000006 :\n",
      "\t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n",
      "\t|- 17262 17241 17262 17262 (bash) 0 0 11550720 585 /bin/bash -c /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000006/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000006 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000004_0 6 1>/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000006/stdout 2>/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000006/stderr  \n",
      "\t|- 17283 17262 17262 17262 (java) 454 29 1947656192 57675 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000006/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000006 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000004_0 6 \n",
      "\t|- 17457 17283 17262 17262 (java) 0 0 1947656192 57675 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000006/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000006 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000004_0 6 \n",
      "\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "19/02/08 07:36:29 INFO mapreduce.Job: Task Id : attempt_1549545179555_0025_m_000005_0, Status : FAILED\n",
      "Container [pid=17246,containerID=container_1549545179555_0025_01_000007] is running beyond virtual memory limits. Current usage: 494.6 MB of 1 GB physical memory used; 3.6 GB of 2.1 GB virtual memory used. Killing container.\n",
      "Dump of the process-tree for container_1549545179555_0025_01_000007 :\n",
      "\t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n",
      "\t|- 17246 17242 17246 17246 (bash) 0 0 11550720 580 /bin/bash -c /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000007/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000007 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000005_0 7 1>/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000007/stdout 2>/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000007/stderr  \n",
      "\t|- 17282 17246 17246 17246 (java) 451 24 1946329088 63023 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000007/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000007 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000005_0 7 \n",
      "\t|- 17458 17282 17246 17246 (java) 0 0 1946329088 63023 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0025/container_1549545179555_0025_01_000007/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0025/container_1549545179555_0025_01_000007 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 38586 attempt_1549545179555_0025_m_000005_0 7 \n",
      "\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "19/02/08 07:37:04 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "19/02/08 07:37:05 INFO mapreduce.Job:  map 58% reduce 0%\n",
      "19/02/08 07:37:10 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "19/02/08 07:37:16 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "19/02/08 07:37:19 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/02/08 07:37:33 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/02/08 07:37:34 INFO mapreduce.Job: Job job_1549545179555_0025 completed successfully\n",
      "19/02/08 07:37:34 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=104544528\n",
      "\t\tFILE: Number of bytes written=210354561\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=97157820\n",
      "\t\tHDFS: Number of bytes written=76750109\n",
      "\t\tHDFS: Number of read operations=27\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=6\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=13\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tOther local map tasks=7\n",
      "\t\tData-local map tasks=6\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=401964\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=60162\n",
      "\t\tTotal time spent by all map tasks (ms)=401964\n",
      "\t\tTotal time spent by all reduce tasks (ms)=60162\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=401964\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=60162\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=411611136\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=61605888\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3898372\n",
      "\t\tMap output records=3898372\n",
      "\t\tMap output bytes=96747742\n",
      "\t\tMap output materialized bytes=104544618\n",
      "\t\tInput split bytes=639\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=852338\n",
      "\t\tReduce shuffle bytes=104544618\n",
      "\t\tReduce input records=3898372\n",
      "\t\tReduce output records=3472203\n",
      "\t\tSpilled Records=7796744\n",
      "\t\tShuffled Maps =18\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=18\n",
      "\t\tGC time elapsed (ms)=2649\n",
      "\t\tCPU time spent (ms)=88340\n",
      "\t\tPhysical memory (bytes) snapshot=2365353984\n",
      "\t\tVirtual memory (bytes) snapshot=17675075584\n",
      "\t\tTotal committed heap usage (bytes)=1765801984\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tWiki stats\n",
      "\t\tidf words=426169\n",
      "\t\ttf words=3472203\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=97157181\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=76750109\n",
      "19/02/08 07:37:34 INFO streaming.StreamJob: Output directory: tf-idf-out\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUT_DIR_TF=\"tf_result\"\n",
    "OUT_DIR_IDF=\"idf_result\"\n",
    "#OUT_DIR_TF_IDF=\"tf_idf_result_\"$(date +\"%s%6N\")\n",
    "OUT_DIR_TF_IDF=\"tf-idf-out\"\n",
    "NUM_REDUCERS=3\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR_TF_IDF} > /dev/null\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D stream.num.reduce.output.fields=2 \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-D mapreduce.partition.keycomparator.options=-k1,2 \\\n",
    "-D mapreduce.job.name=\"Tf-Idf tf*idf task\" \\\n",
    "-D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "-files mapper-tf-idf.py,reducer-tf-idf.py \\\n",
    "-mapper \"python mapper-tf-idf.py\" \\\n",
    "-reducer \"python reducer-tf-idf.py\" \\\n",
    "-input ${OUT_DIR_TF},${OUT_DIR_IDF} \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-output ${OUT_DIR_TF_IDF} > /dev/null\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   1 jovyan supergroup          0 2019-02-08 07:37 tf-idf-out/_SUCCESS\r\n",
      "-rw-r--r--   1 jovyan supergroup   25513964 2019-02-08 07:37 tf-idf-out/part-00000\r\n",
      "-rw-r--r--   1 jovyan supergroup   26007862 2019-02-08 07:37 tf-idf-out/part-00001\r\n",
      "-rw-r--r--   1 jovyan supergroup   25228283 2019-02-08 07:37 tf-idf-out/part-00002\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls tf-idf-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -getmerge tf-idf-out/* tf-idf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3472203 10416609 76750109 tf-idf.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc tf-idf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labor\t12\t0.000351\r\n"
     ]
    }
   ],
   "source": [
    "!grep -E 'labor\\W12\\W' tf-idf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labor\t12\t0.000351\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -text tf-idf-out/* | grep -E 'labor\\W12\\W'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
