{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper-tf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-tf.py\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "path = 'stop_words_en.txt'\n",
    "tagWord = 'wordN'\n",
    "tagWords = 'totalWordsNt'\n",
    "\n",
    "stopWords = set()\n",
    "\n",
    "with open(path) as stopWordsFile:\n",
    "    for line in stopWordsFile:\n",
    "        try:         \n",
    "            stopWords.add(unicode(line.strip().lower()))\n",
    "        except ValueError as e:\n",
    "            continue\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = unicode(line.strip()).split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        print(\"reporter:counter:Wiki stats,Wrong articles found,%d\" % 1, file=sys.stderr)\n",
    "        continue\n",
    "\n",
    "    text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "    words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "    \n",
    "    wordsInArticle = 0\n",
    "    \n",
    "    for word in words:\n",
    "        word = word.strip(string.punctuation).lower()\n",
    "        if word == \"\":\n",
    "            print(\"reporter:counter:Wiki stats,Null words found,%d\" % 1, file=sys.stderr)\n",
    "            continue\n",
    "        if word in stopWords:\n",
    "            print(\"reporter:counter:Wiki stats,Stop words found,%d\" % 1, file=sys.stderr)\n",
    "            continue\n",
    "        wordsInArticle += 1\n",
    "        print(\"reporter:counter:Wiki stats,Total words found,%d\" % 1, file=sys.stderr)\n",
    "        print(article_id, tagWord, word, 1, sep='\\t')\n",
    "\n",
    "    print(article_id, tagWords, \"-\", wordsInArticle, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer-tf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-tf.py\n",
    "import sys\n",
    "\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "current_article_id = None\n",
    "current_word = None\n",
    "word_sum, wordsInArticle = 0, 0\n",
    "\n",
    "tagWord = 'wordN'\n",
    "tagWords = 'totalWordsNt'\n",
    "tagTf = 'tf'\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, tag, word, count = unicode(line.strip()).split('\\t')\n",
    "        #print \"after parse:\", article_id, word, count\n",
    "        if article_id == \"\" or tag == \"\" or word == \"\" or count == \"\":   # sanity check\n",
    "            continue\n",
    "        count = int(count)\n",
    "    except ValueError as e:\n",
    "        #print \"Parse error:\", e\n",
    "        continue\n",
    "\n",
    "    if current_article_id != article_id:\n",
    "        if tag == tagWords: # get total words in article\n",
    "            wordsInArticle = count\n",
    "            continue\n",
    "        if current_article_id != None:\n",
    "            print \"%s\\t%s\\t%s\\t%f\" % (current_word, tagTf, current_article_id, 1.0*word_sum/wordsInArticle)\n",
    "            word_sum = 0\n",
    "            current_word = word\n",
    "        current_article_id = article_id\n",
    "        #print \"wordsInArticle:\", word, wordsInArticle\n",
    "        \n",
    "    if current_word != word:\n",
    "        if current_word != None:\n",
    "            print \"%s\\t%s\\t%s\\t%f\" % (current_word, tagTf, current_article_id, 1.0*word_sum/wordsInArticle)\n",
    "            #print \"%s\\t%s\\t%d\\t%d\" % (current_article_id, current_word, word_sum, wordsInArticle)\n",
    "        word_sum = 0\n",
    "        current_word = word\n",
    "    word_sum += count\n",
    "\n",
    "if current_article_id:\n",
    "    print \"%s\\t%s\\t%s\\t%f\" % (current_word, tagTf, current_article_id, 1.0*word_sum/wordsInArticle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper-idf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-idf.py\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "for line in sys.stdin:\n",
    "    word, _, _, _ = unicode(line.strip()).split('\\t')\n",
    "    print(\"reporter:counter:Idf stats,Total words found,%d\" % 1, file=sys.stderr)\n",
    "    print(word, 1, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer-idf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-idf.py\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "current_key = None\n",
    "word_sum = 0\n",
    "tag = \"idf\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        key, count = unicode(line.strip()).split('\\t', 1)\n",
    "        if key == \"\" or count == \"\":    # sanity check\n",
    "            continue\n",
    "        count = int(count)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    if current_key != key:\n",
    "        if current_key:\n",
    "            print \"%s\\t%s\\t%f\" % (current_key, tag, (1.0/log(1.0 + word_sum)))\n",
    "        word_sum = 0\n",
    "        current_key = key\n",
    "    word_sum += count\n",
    "\n",
    "if current_key:\n",
    "    print \"%s\\t%s\\t%f\" % (current_key, tag, (1.0/log(1.0 + word_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper-tf-idf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper-tf-idf.py\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "tf = \"tf\"\n",
    "idf = \"idf\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    inVal = unicode(line.strip()).split('\\t')\n",
    "    if len(inVal) > 4:\n",
    "        print(\"reporter:counter:Wiki stats,tf idf > 4 parsing errors,%d\" % 1, file=sys.stderr)\n",
    "        continue\n",
    "    try:\n",
    "        if inVal[1] == idf: # idf data\n",
    "            print(\"reporter:counter:Wiki stats,idf words,%d\" % 1, file=sys.stderr)\n",
    "            print(inVal[0], idf, float(inVal[2]), sep='\\t')\n",
    "            \n",
    "        elif inVal[1] == tf: # tf data\n",
    "            print(\"reporter:counter:Wiki stats,tf words,%d\" % 1, file=sys.stderr)\n",
    "            print(inVal[0], tf, inVal[2], float(inVal[3]), sep='\\t')\n",
    "            \n",
    "        else:\n",
    "            print(\"reporter:counter:Wiki stats,tf idf last else parsing errors,%d\" % 1, file=sys.stderr)\n",
    "    except ValueError as e:\n",
    "        print(\"reporter:counter:Wiki stats,tf idf to float errors,%d\" % 1, file=sys.stderr)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer-tf-idf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer-tf-idf.py\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')  # required to convert to unicode\n",
    "\n",
    "tf = \"tf\"\n",
    "idf = \"idf\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    inVal = unicode(line.strip()).split('\\t')\n",
    "    if (len(inVal) < 2 and len(inVal) > 4):      # sanity check\n",
    "        continue\n",
    "    try:\n",
    "        if inVal[1] == idf:\n",
    "            idfWord, _, idfDt = inVal\n",
    "            idfDt = float(idfDt)\n",
    "\n",
    "        if inVal[1] == tf:\n",
    "            tfWord, _, article_id, tfNt = inVal\n",
    "            tfNt = float(tfNt)\n",
    "            if idfWord == tfWord:\n",
    "                print \"%s\\t%s\\t%f\" % (idfWord, article_id, tfNt*idfDt)\n",
    "\n",
    "    except ValueError as e:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labor\t12\t0.000351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `tf_result1549615133620370': No such file or directory\n",
      "19/02/08 08:38:57 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/02/08 08:38:57 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/02/08 08:38:58 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "19/02/08 08:38:58 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/02/08 08:38:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1549545179555_0026\n",
      "19/02/08 08:38:59 INFO impl.YarnClientImpl: Submitted application application_1549545179555_0026\n",
      "19/02/08 08:38:59 INFO mapreduce.Job: The url to track the job: http://e5801941607d:8088/proxy/application_1549545179555_0026/\n",
      "19/02/08 08:38:59 INFO mapreduce.Job: Running job: job_1549545179555_0026\n",
      "19/02/08 08:39:06 INFO mapreduce.Job: Job job_1549545179555_0026 running in uber mode : false\n",
      "19/02/08 08:39:06 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/02/08 08:39:23 INFO mapreduce.Job:  map 18% reduce 0%\n",
      "19/02/08 08:39:29 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "19/02/08 08:39:35 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "19/02/08 08:39:41 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "19/02/08 08:39:47 INFO mapreduce.Job:  map 52% reduce 0%\n",
      "19/02/08 08:39:53 INFO mapreduce.Job:  map 55% reduce 0%\n",
      "19/02/08 08:39:59 INFO mapreduce.Job:  map 64% reduce 0%\n",
      "19/02/08 08:40:05 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "19/02/08 08:40:08 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "19/02/08 08:40:09 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/02/08 08:40:26 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "19/02/08 08:40:28 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "19/02/08 08:40:32 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "19/02/08 08:40:33 INFO mapreduce.Job:  map 100% reduce 94%\n",
      "19/02/08 08:40:34 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/02/08 08:40:35 INFO mapreduce.Job: Job job_1549545179555_0026 completed successfully\n",
      "19/02/08 08:40:36 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=316786759\n",
      "\t\tFILE: Number of bytes written=475859962\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=76874501\n",
      "\t\tHDFS: Number of bytes written=87166718\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=121435\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=66362\n",
      "\t\tTotal time spent by all map tasks (ms)=121435\n",
      "\t\tTotal time spent by all reduce tasks (ms)=66362\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=121435\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=66362\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=124349440\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=67954688\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4100\n",
      "\t\tMap output records=6975047\n",
      "\t\tMap output bytes=144435146\n",
      "\t\tMap output materialized bytes=158385291\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3476303\n",
      "\t\tReduce shuffle bytes=158385291\n",
      "\t\tReduce input records=6975047\n",
      "\t\tReduce output records=3472203\n",
      "\t\tSpilled Records=20925141\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=2324\n",
      "\t\tCPU time spent (ms)=219390\n",
      "\t\tPhysical memory (bytes) snapshot=1317978112\n",
      "\t\tVirtual memory (bytes) snapshot=9831493632\n",
      "\t\tTotal committed heap usage (bytes)=933232640\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tWiki stats\n",
      "\t\tNull words found=72\n",
      "\t\tStop words found=4966298\n",
      "\t\tTotal words found=6970947\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=76874273\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=87166718\n",
      "19/02/08 08:40:36 INFO streaming.StreamJob: Output directory: tf_result1549615133620370\n",
      "rm: `idf_result1549615236425786': No such file or directory\n",
      "19/02/08 08:40:43 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/02/08 08:40:43 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/02/08 08:40:44 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "19/02/08 08:40:44 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "19/02/08 08:40:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1549545179555_0027\n",
      "19/02/08 08:40:45 INFO impl.YarnClientImpl: Submitted application application_1549545179555_0027\n",
      "19/02/08 08:40:45 INFO mapreduce.Job: The url to track the job: http://e5801941607d:8088/proxy/application_1549545179555_0027/\n",
      "19/02/08 08:40:45 INFO mapreduce.Job: Running job: job_1549545179555_0027\n",
      "19/02/08 08:40:52 INFO mapreduce.Job: Job job_1549545179555_0027 running in uber mode : false\n",
      "19/02/08 08:40:52 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/02/08 08:41:10 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/02/08 08:41:21 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "19/02/08 08:41:22 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "19/02/08 08:41:23 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/02/08 08:41:23 INFO mapreduce.Job: Job job_1549545179555_0027 completed successfully\n",
      "19/02/08 08:41:24 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=42248818\n",
      "\t\tFILE: Number of bytes written=85334927\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=87167084\n",
      "\t\tHDFS: Number of bytes written=9990463\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=47367\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=26723\n",
      "\t\tTotal time spent by all map tasks (ms)=47367\n",
      "\t\tTotal time spent by all reduce tasks (ms)=26723\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=47367\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=26723\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=48503808\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=27364352\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3472203\n",
      "\t\tMap output records=3472203\n",
      "\t\tMap output bytes=35304384\n",
      "\t\tMap output materialized bytes=42248854\n",
      "\t\tInput split bytes=366\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=426169\n",
      "\t\tReduce shuffle bytes=42248854\n",
      "\t\tReduce input records=3472203\n",
      "\t\tReduce output records=426169\n",
      "\t\tSpilled Records=6944406\n",
      "\t\tShuffled Maps =9\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=9\n",
      "\t\tGC time elapsed (ms)=935\n",
      "\t\tCPU time spent (ms)=31230\n",
      "\t\tPhysical memory (bytes) snapshot=1378148352\n",
      "\t\tVirtual memory (bytes) snapshot=11790659584\n",
      "\t\tTotal committed heap usage (bytes)=932708352\n",
      "\tIdf stats\n",
      "\t\tTotal words found=3472203\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=87166718\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9990463\n",
      "19/02/08 08:41:24 INFO streaming.StreamJob: Output directory: idf_result1549615236425786\n",
      "rm: `tf-idf-out1549615284344545': No such file or directory\n",
      "19/02/08 08:41:27 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/02/08 08:41:28 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/02/08 08:41:28 INFO mapred.FileInputFormat: Total input files to process : 6\n",
      "19/02/08 08:41:28 INFO mapreduce.JobSubmitter: number of splits:6\n",
      "19/02/08 08:41:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1549545179555_0028\n",
      "19/02/08 08:41:29 INFO impl.YarnClientImpl: Submitted application application_1549545179555_0028\n",
      "19/02/08 08:41:29 INFO mapreduce.Job: The url to track the job: http://e5801941607d:8088/proxy/application_1549545179555_0028/\n",
      "19/02/08 08:41:29 INFO mapreduce.Job: Running job: job_1549545179555_0028\n",
      "19/02/08 08:41:35 INFO mapreduce.Job: Job job_1549545179555_0028 running in uber mode : false\n",
      "19/02/08 08:41:35 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/02/08 08:41:59 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "19/02/08 08:42:05 INFO mapreduce.Job:  map 49% reduce 0%\n",
      "19/02/08 08:42:06 INFO mapreduce.Job:  map 64% reduce 0%\n",
      "19/02/08 08:42:08 INFO mapreduce.Job: Task Id : attempt_1549545179555_0028_m_000003_0, Status : FAILED\n",
      "Container [pid=20400,containerID=container_1549545179555_0028_01_000005] is running beyond virtual memory limits. Current usage: 491.9 MB of 1 GB physical memory used; 3.6 GB of 2.1 GB virtual memory used. Killing container.\n",
      "Dump of the process-tree for container_1549545179555_0028_01_000005 :\n",
      "\t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n",
      "\t|- 20400 20394 20400 20400 (bash) 0 0 11550720 521 /bin/bash -c /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0028/container_1549545179555_0028_01_000005/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0028/container_1549545179555_0028_01_000005 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 44419 attempt_1549545179555_0028_m_000003_0 5 1>/opt/hadoop/logs/userlogs/application_1549545179555_0028/container_1549545179555_0028_01_000005/stdout 2>/opt/hadoop/logs/userlogs/application_1549545179555_0028/container_1549545179555_0028_01_000005/stderr  \n",
      "\t|- 20615 20437 20400 20400 (java) 0 0 1947828224 62698 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0028/container_1549545179555_0028_01_000005/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0028/container_1549545179555_0028_01_000005 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 44419 attempt_1549545179555_0028_m_000003_0 5 \n",
      "\t|- 20437 20400 20400 20400 (java) 468 26 1947828224 62698 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1549545179555_0028/container_1549545179555_0028_01_000005/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1549545179555_0028/container_1549545179555_0028_01_000005 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.172.1.2 44419 attempt_1549545179555_0028_m_000003_0 5 \n",
      "\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "19/02/08 08:42:10 INFO mapreduce.Job:  map 76% reduce 0%\n",
      "19/02/08 08:42:11 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "19/02/08 08:42:57 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/02/08 08:43:04 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "19/02/08 08:43:06 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "19/02/08 08:43:07 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/02/08 08:43:08 INFO mapreduce.Job: Job job_1549545179555_0028 completed successfully\n",
      "19/02/08 08:43:09 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=104544528\n",
      "\t\tFILE: Number of bytes written=210354993\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=97157916\n",
      "\t\tHDFS: Number of bytes written=76750109\n",
      "\t\tHDFS: Number of read operations=27\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=1\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=8\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tOther local map tasks=2\n",
      "\t\tData-local map tasks=6\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=261435\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=166130\n",
      "\t\tTotal time spent by all map tasks (ms)=261435\n",
      "\t\tTotal time spent by all reduce tasks (ms)=166130\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=261435\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=166130\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=267709440\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=170117120\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3898372\n",
      "\t\tMap output records=3898372\n",
      "\t\tMap output bytes=96747742\n",
      "\t\tMap output materialized bytes=104544618\n",
      "\t\tInput split bytes=735\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=852338\n",
      "\t\tReduce shuffle bytes=104544618\n",
      "\t\tReduce input records=3898372\n",
      "\t\tReduce output records=3472203\n",
      "\t\tSpilled Records=7796744\n",
      "\t\tShuffled Maps =18\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=18\n",
      "\t\tGC time elapsed (ms)=2492\n",
      "\t\tCPU time spent (ms)=103720\n",
      "\t\tPhysical memory (bytes) snapshot=2300956672\n",
      "\t\tVirtual memory (bytes) snapshot=17680842752\n",
      "\t\tTotal committed heap usage (bytes)=1699217408\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tWiki stats\n",
      "\t\tidf words=426169\n",
      "\t\ttf words=3472203\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=97157181\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=76750109\n",
      "19/02/08 08:43:09 INFO streaming.StreamJob: Output directory: tf-idf-out1549615284344545\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUT_DIR_TF=\"tf_result\"$(date +\"%s%6N\")\n",
    "\n",
    "NUM_REDUCERS=3\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR_TF} > /dev/null\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D stream.num.reduce.output.fields=3 \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-D mapreduce.partition.keycomparator.options=-k1,3 \\\n",
    "-D mapreduce.job.name=\"Tf-Idf tf task\" \\\n",
    "-D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "-files mapper-tf.py,reducer-tf.py,/datasets/stop_words_en.txt  \\\n",
    "-mapper \"python mapper-tf.py\" \\\n",
    "-reducer \"python reducer-tf.py\" \\\n",
    "-input /data/wiki/en_articles_part \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-output ${OUT_DIR_TF} > /dev/null\n",
    "\n",
    "OUT_DIR_IDF=\"idf_result\"$(date +\"%s%6N\")\n",
    "NUM_REDUCERS=3\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR_IDF} > /dev/null\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"Tf-Idf idf task\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -files mapper-idf.py,reducer-idf.py \\\n",
    "    -mapper \"python mapper-idf.py\" \\\n",
    "    -reducer \"python reducer-idf.py\" \\\n",
    "    -input ${OUT_DIR_TF} \\\n",
    "    -output ${OUT_DIR_IDF} > /dev/null\n",
    "\n",
    "\n",
    "OUT_DIR_TF_IDF=\"tf-idf-out\"$(date +\"%s%6N\")\n",
    "NUM_REDUCERS=3\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR_TF_IDF} > /dev/null\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D stream.num.reduce.output.fields=2 \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-D mapreduce.partition.keycomparator.options=-k1,2 \\\n",
    "-D mapreduce.job.name=\"Tf-Idf tf*idf task\" \\\n",
    "-D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "-files mapper-tf-idf.py,reducer-tf-idf.py \\\n",
    "-mapper \"python mapper-tf-idf.py\" \\\n",
    "-reducer \"python reducer-tf-idf.py\" \\\n",
    "-input ${OUT_DIR_TF},${OUT_DIR_IDF} \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-output ${OUT_DIR_TF_IDF} > /dev/null\n",
    "\n",
    "hdfs dfs -text ${OUT_DIR_TF_IDF}/* | grep -E 'labor\\W12\\W'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
